{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classificator problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing\n",
    "from fuzzywuzzy import fuzz # fuzzy logic, compare strings\n",
    "from collections import defaultdict # dictionary with default value\n",
    "# Functions I created for this task\n",
    "from functions import find_similar_columns, investigate_similar_columns, swap_columns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # label encoder\n",
    "from sklearn.preprocessing import OneHotEncoder # one hot encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRAUDE</th>\n",
       "      <th>VALOR</th>\n",
       "      <th>HORA_AUX</th>\n",
       "      <th>Dist_max_NAL</th>\n",
       "      <th>Canal1</th>\n",
       "      <th>FECHA</th>\n",
       "      <th>COD_PAIS</th>\n",
       "      <th>CANAL</th>\n",
       "      <th>DIASEM</th>\n",
       "      <th>DIAMES</th>\n",
       "      <th>...</th>\n",
       "      <th>INGRESOS</th>\n",
       "      <th>EGRESOS</th>\n",
       "      <th>NROPAISES</th>\n",
       "      <th>Dist_Sum_INTER</th>\n",
       "      <th>Dist_Mean_INTER</th>\n",
       "      <th>Dist_Max_INTER</th>\n",
       "      <th>NROCIUDADES</th>\n",
       "      <th>Dist_Mean_NAL</th>\n",
       "      <th>Dist_HOY</th>\n",
       "      <th>Dist_sum_NAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000000001</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>659.13</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150501</td>\n",
       "      <td>US</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>474.94</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>5224.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000000002</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>594.77</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150515</td>\n",
       "      <td>US</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>5643700.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>289.99</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>2029.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000000003</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>659.13</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150501</td>\n",
       "      <td>US</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>474.94</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>5224.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000000004</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>659.13</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150501</td>\n",
       "      <td>US</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>474.94</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>5224.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000000005</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150510</td>\n",
       "      <td>CR</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1482.35</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FRAUDE  VALOR  HORA_AUX  Dist_max_NAL   Canal1     FECHA COD_PAIS   \n",
       "id                                                                              \n",
       "9000000001       1    0.0        13        659.13  ATM_INT  20150501       US  \\\n",
       "9000000002       1    0.0        17        594.77  ATM_INT  20150515       US   \n",
       "9000000003       1    0.0        13        659.13  ATM_INT  20150501       US   \n",
       "9000000004       1    0.0        13        659.13  ATM_INT  20150501       US   \n",
       "9000000005       1    0.0         0          1.00  ATM_INT  20150510       CR   \n",
       "\n",
       "              CANAL  DIASEM  DIAMES  ...   INGRESOS    EGRESOS NROPAISES   \n",
       "id                                   ...                                   \n",
       "9000000001  ATM_INT       5       1  ...  1200000.0  1200000.0         1  \\\n",
       "9000000002  ATM_INT       5      15  ...  5643700.0   500000.0         1   \n",
       "9000000003  ATM_INT       5       1  ...  1200000.0  1200000.0         1   \n",
       "9000000004  ATM_INT       5       1  ...  1200000.0  1200000.0         1   \n",
       "9000000005  ATM_INT       0      10  ...        0.0        0.0         1   \n",
       "\n",
       "           Dist_Sum_INTER  Dist_Mean_INTER  Dist_Max_INTER  NROCIUDADES   \n",
       "id                                                                        \n",
       "9000000001            NaN              NaN             NaN            6  \\\n",
       "9000000002            NaN              NaN             NaN            5   \n",
       "9000000003            NaN              NaN             NaN            6   \n",
       "9000000004            NaN              NaN             NaN            6   \n",
       "9000000005            NaN              NaN             NaN            1   \n",
       "\n",
       "            Dist_Mean_NAL  Dist_HOY  Dist_sum_NAL  \n",
       "id                                                 \n",
       "9000000001         474.94   4552.41       5224.36  \n",
       "9000000002         289.99   4552.41       2029.90  \n",
       "9000000003         474.94   4552.41       5224.36  \n",
       "9000000004         474.94   4552.41       5224.36  \n",
       "9000000005            NaN   1482.35          1.00  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FRAUDE</th>\n",
       "      <th>VALOR</th>\n",
       "      <th>HORA_AUX</th>\n",
       "      <th>Dist_max_COL</th>\n",
       "      <th>Dist_max_INTER</th>\n",
       "      <th>Canal1</th>\n",
       "      <th>FECHA_FRAUDE</th>\n",
       "      <th>COD_PAIS</th>\n",
       "      <th>CANAL</th>\n",
       "      <th>FECHA</th>\n",
       "      <th>...</th>\n",
       "      <th>Dist_Mean_INTER</th>\n",
       "      <th>Dist_Max_INTER</th>\n",
       "      <th>NROCIUDADES</th>\n",
       "      <th>Dist_Sum_NAL</th>\n",
       "      <th>Dist_Mean_NAL</th>\n",
       "      <th>Dist_HOY</th>\n",
       "      <th>Dist_sum_NAL</th>\n",
       "      <th>Dist_mean_NAL</th>\n",
       "      <th>Dist_sum_INTER</th>\n",
       "      <th>Dist_mean_INTER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98523068</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42230.09</td>\n",
       "      <td>18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>20150515</td>\n",
       "      <td>US</td>\n",
       "      <td>POS</td>\n",
       "      <td>20150515</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300237898</th>\n",
       "      <td>NaN</td>\n",
       "      <td>143202.65</td>\n",
       "      <td>20</td>\n",
       "      <td>614.04</td>\n",
       "      <td>7632.97</td>\n",
       "      <td>POS</td>\n",
       "      <td>20150506</td>\n",
       "      <td>US</td>\n",
       "      <td>MCI</td>\n",
       "      <td>20150506</td>\n",
       "      <td>...</td>\n",
       "      <td>6092.69</td>\n",
       "      <td>7632.97</td>\n",
       "      <td>2</td>\n",
       "      <td>1228.07</td>\n",
       "      <td>614.04</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>1228.07</td>\n",
       "      <td>614.04</td>\n",
       "      <td>24370.75</td>\n",
       "      <td>6092.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943273308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>243591.25</td>\n",
       "      <td>2</td>\n",
       "      <td>286.84</td>\n",
       "      <td>2443.14</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150517</td>\n",
       "      <td>EC</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150517</td>\n",
       "      <td>...</td>\n",
       "      <td>1743.52</td>\n",
       "      <td>2443.14</td>\n",
       "      <td>7</td>\n",
       "      <td>1944.35</td>\n",
       "      <td>138.88</td>\n",
       "      <td>5083.41</td>\n",
       "      <td>1944.35</td>\n",
       "      <td>138.88</td>\n",
       "      <td>6974.09</td>\n",
       "      <td>1743.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951645809</th>\n",
       "      <td>NaN</td>\n",
       "      <td>238267.40</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150508</td>\n",
       "      <td>EC</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150508</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904.81</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963797516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>490403.58</td>\n",
       "      <td>13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150501</td>\n",
       "      <td>US</td>\n",
       "      <td>ATM_INT</td>\n",
       "      <td>20150501</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4552.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           FRAUDE      VALOR  HORA_AUX  Dist_max_COL  Dist_max_INTER   Canal1   \n",
       "id                                                                              \n",
       "98523068      NaN   42230.09        18          1.00            1.00      POS  \\\n",
       "300237898     NaN  143202.65        20        614.04         7632.97      POS   \n",
       "943273308     NaN  243591.25         2        286.84         2443.14  ATM_INT   \n",
       "951645809     NaN  238267.40        20          1.00            1.00  ATM_INT   \n",
       "963797516     NaN  490403.58        13          1.00            1.00  ATM_INT   \n",
       "\n",
       "           FECHA_FRAUDE COD_PAIS    CANAL     FECHA  ...  Dist_Mean_INTER   \n",
       "id                                                   ...                    \n",
       "98523068       20150515       US      POS  20150515  ...              NaN  \\\n",
       "300237898      20150506       US      MCI  20150506  ...          6092.69   \n",
       "943273308      20150517       EC  ATM_INT  20150517  ...          1743.52   \n",
       "951645809      20150508       EC  ATM_INT  20150508  ...              NaN   \n",
       "963797516      20150501       US  ATM_INT  20150501  ...              NaN   \n",
       "\n",
       "           Dist_Max_INTER  NROCIUDADES  Dist_Sum_NAL Dist_Mean_NAL Dist_HOY   \n",
       "id                                                                            \n",
       "98523068              NaN            1           NaN           NaN  4552.41  \\\n",
       "300237898         7632.97            2       1228.07        614.04  4552.41   \n",
       "943273308         2443.14            7       1944.35        138.88  5083.41   \n",
       "951645809             NaN            1           NaN           NaN   904.81   \n",
       "963797516             NaN            1           NaN           NaN  4552.41   \n",
       "\n",
       "           Dist_sum_NAL  Dist_mean_NAL  Dist_sum_INTER  Dist_mean_INTER  \n",
       "id                                                                       \n",
       "98523068           1.00           1.00            1.00             1.00  \n",
       "300237898       1228.07         614.04        24370.75          6092.69  \n",
       "943273308       1944.35         138.88         6974.09          1743.52  \n",
       "951645809          1.00           1.00            1.00             1.00  \n",
       "963797516          1.00           1.00            1.00             1.00  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and inspect train and test datasets\n",
    "train = pd.read_csv('../Data/train.csv', index_col=0)\n",
    "test = pd.read_csv('../Data/test.csv', index_col=0)\n",
    "\n",
    "display(train.head())\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable from the features\n",
    "y_train = train['FRAUDE']\n",
    "X_train = train.drop('FRAUDE', axis=1)\n",
    "\n",
    "X_test = test.drop('FRAUDE', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analyze the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing is to check what the columns in common between `X_train` and `X_test` are. Also it is useful to check the columns belonging to `X_train` that are not in `X_test` and viceversa. The two sets used should have exactly the same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for features\n",
    "train_columns = list(X_train.columns)\n",
    "test_columns = list(X_test.columns)\n",
    "\n",
    "# Extract columns in common\n",
    "common_columns = list(set(train_columns) & set(test_columns))\n",
    "\n",
    "# Extract columns in train but not in test and viceversa\n",
    "train_not_test = list(set(train_columns) - set(test_columns))\n",
    "test_not_train = list(set(test_columns) - set(train_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in common:  ['NROPAISES', 'Dist_Sum_INTER', 'SEGMENTO', 'COD_PAIS', 'NROCIUDADES', 'HORA_AUX', 'DIASEM', 'Dist_Max_INTER', 'OFICINA_VIN', 'Dist_HOY', 'Dist_Mean_NAL', 'CANAL', 'FECHA', 'SEXO', 'EDAD', 'EGRESOS', 'DIAMES', 'VALOR', 'INGRESOS', 'FECHA_VIN', 'Dist_sum_NAL', 'Canal1', 'Dist_Mean_INTER']\n",
      "Columns in train but not in test:  ['Dist_max_NAL']\n",
      "Columns in test but not in train:  ['FECHA_FRAUDE', 'Dist_Sum_NAL', 'Dist_sum_INTER', 'Dist_mean_NAL', 'Dist_max_INTER', 'Dist_mean_INTER', 'Dist_max_COL']\n"
     ]
    }
   ],
   "source": [
    "print('Columns in common: ', common_columns)\n",
    "print('Columns in train but not in test: ', train_not_test)\n",
    "print('Columns in test but not in train: ', test_not_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Investigate columns with similar names\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are columns \"FECHA\" and \"FECHA_FRAUDE\" equal?  True\n"
     ]
    }
   ],
   "source": [
    "# Analyze equality of columns \"FECHA\" and \"FECHA_FRAUDE\"\n",
    "fecha_equals_fechafraude = X_test[\"FECHA\"].equals(X_test[\"FECHA_FRAUDE\"])\n",
    "\n",
    "# If they're equal, drop \"FECHA_FRAUDE\" from X_test since it's redundant\n",
    "if fecha_equals_fechafraude:\n",
    "    X_test = X_test.drop('FECHA_FRAUDE', axis=1)\n",
    "\n",
    "# Print if they're equal\n",
    "print('Are columns \"FECHA\" and \"FECHA_FRAUDE\" equal? ', fecha_equals_fechafraude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar column names in test\n",
    "similar_columns = find_similar_columns(X_test, threshold=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Dist_max_INTER', 'Dist_Sum_INTER', 'Dist_Mean_INTER', 'Dist_Sum_NAL', 'Dist_Mean_NAL'])\n",
      "{'Dist_max_INTER': ['Dist_max_INTER', 'Dist_Max_INTER'], 'Dist_Sum_INTER': ['Dist_Sum_INTER', 'Dist_sum_INTER'], 'Dist_Mean_INTER': ['Dist_Mean_INTER', 'Dist_mean_INTER'], 'Dist_Sum_NAL': ['Dist_Sum_NAL', 'Dist_sum_NAL'], 'Dist_Mean_NAL': ['Dist_Mean_NAL', 'Dist_mean_NAL']}\n"
     ]
    }
   ],
   "source": [
    "# Print similar_columns\n",
    "print(similar_columns.keys())\n",
    "print(similar_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_max_INTER\n",
      "Column to drop:  Dist_Max_INTER\n",
      "Column Dist_max_INTER is not in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_sum_INTER\n",
      "Column to drop:  Dist_Sum_INTER\n",
      "Column Dist_sum_INTER is not in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_mean_INTER\n",
      "Column to drop:  Dist_Mean_INTER\n",
      "Column Dist_mean_INTER is not in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_sum_NAL\n",
      "Column to drop:  Dist_Sum_NAL\n",
      "Column Dist_sum_NAL is in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_mean_NAL\n",
      "Column to drop:  Dist_Mean_NAL\n",
      "Column Dist_mean_NAL is not in common_columns\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the keys in similar_columns and apply the function investigate_similar_columns\n",
    "for key in similar_columns.keys():\n",
    "    print(\"\\n\")\n",
    "    investigate_similar_columns(X_test, similar_columns[key][0], similar_columns[key][1],\n",
    "                                common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function swap_columns in module functions:\n",
      "\n",
      "swap_columns(df: pandas.core.frame.DataFrame, first_column: str, second_column: str)\n",
      "    Function to swap two column names in a dataframe\n",
      "    \n",
      "    Args:\n",
      "        df (pd.DataFrame): Dataframe to swap columns in\n",
      "        first_column (str): Name of the first column to swap\n",
      "        second_column (str): Name of the second column to swap\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: Dataframe with swapped columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(swap_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the keys of similar_columns excluding the key \"Dist_Sum_NAL\"\n",
    "keys_to_swap = [key for key in similar_columns.keys() if key != 'Dist_Sum_NAL']\n",
    "\n",
    "# Iterate over the elements in keys_to_swap and apply the function swap_columns to X_test\n",
    "for key in keys_to_swap:\n",
    "    X_test = swap_columns(X_test, similar_columns[key][0], similar_columns[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_Max_INTER\n",
      "Column to drop:  Dist_max_INTER\n",
      "Column Dist_Max_INTER is in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_Sum_INTER\n",
      "Column to drop:  Dist_sum_INTER\n",
      "Column Dist_Sum_INTER is in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_Mean_INTER\n",
      "Column to drop:  Dist_mean_INTER\n",
      "Column Dist_Mean_INTER is in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_sum_NAL\n",
      "Column to drop:  Dist_Sum_NAL\n",
      "Column Dist_sum_NAL is in common_columns\n",
      "\n",
      "\n",
      "Columns are equal:  False\n",
      "Non-null values are the same:  True\n",
      "Column with the non-null (imputed) values:  Dist_Mean_NAL\n",
      "Column to drop:  Dist_mean_NAL\n",
      "Column Dist_Mean_NAL is in common_columns\n"
     ]
    }
   ],
   "source": [
    "# Run again to check if the swapping worked\n",
    "# Iterate over the keys in similar_columns and apply the function investigate_similar_columns\n",
    "for key in similar_columns.keys():\n",
    "    print(\"\\n\")\n",
    "    investigate_similar_columns(X_test, similar_columns[key][0], similar_columns[key][1],\n",
    "                                common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For X_train and X_test keep only the columns in common\n",
    "X_train = X_train[common_columns]\n",
    "X_test = X_test[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dist_Mean_NAL      0\n",
       "EDAD               0\n",
       "INGRESOS           0\n",
       "Dist_Max_INTER     0\n",
       "EGRESOS            0\n",
       "FECHA              0\n",
       "VALOR              0\n",
       "Dist_Sum_INTER     0\n",
       "Dist_sum_NAL       0\n",
       "Dist_Mean_INTER    0\n",
       "Dist_HOY           0\n",
       "OFICINA_VIN        0\n",
       "HORA_AUX           0\n",
       "DIAMES             0\n",
       "NROCIUDADES        0\n",
       "NROPAISES          0\n",
       "DIASEM             0\n",
       "FECHA_VIN          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check numerical columns of X_test with null values\n",
    "X_test.select_dtypes(include=['int64', 'float64']).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist_Mean_NAL       457\n",
      "EDAD                 24\n",
      "INGRESOS             24\n",
      "Dist_Max_INTER     1547\n",
      "EGRESOS              24\n",
      "FECHA                 0\n",
      "VALOR                 0\n",
      "Dist_Sum_INTER     1547\n",
      "Dist_sum_NAL          0\n",
      "Dist_Mean_INTER    1547\n",
      "Dist_HOY              0\n",
      "OFICINA_VIN          24\n",
      "HORA_AUX              0\n",
      "DIAMES                0\n",
      "NROCIUDADES           0\n",
      "NROPAISES             0\n",
      "DIASEM                0\n",
      "FECHA_VIN            24\n",
      "dtype: int64\n",
      "(2965, 23)\n",
      "\n",
      "\n",
      "Dist_Mean_NAL      15.413153\n",
      "EDAD                0.809444\n",
      "INGRESOS            0.809444\n",
      "Dist_Max_INTER     52.175379\n",
      "EGRESOS             0.809444\n",
      "FECHA               0.000000\n",
      "VALOR               0.000000\n",
      "Dist_Sum_INTER     52.175379\n",
      "Dist_sum_NAL        0.000000\n",
      "Dist_Mean_INTER    52.175379\n",
      "Dist_HOY            0.000000\n",
      "OFICINA_VIN         0.809444\n",
      "HORA_AUX            0.000000\n",
      "DIAMES              0.000000\n",
      "NROCIUDADES         0.000000\n",
      "NROPAISES           0.000000\n",
      "DIASEM              0.000000\n",
      "FECHA_VIN           0.809444\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check numerical columns of X_train with null values\n",
    "print(X_train.select_dtypes(include=['int64', 'float64']).isnull().sum())\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Percentage of null values in each numeric column\n",
    "print(X_train.select_dtypes(include=['int64', 'float64']).isnull().sum() * 100. / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null percentages\n",
    "null_percentages = X_train.select_dtypes(include=['int64', 'float64']).isnull().sum() * 100. / X_train.shape[0]\n",
    "\n",
    "# Columns with more than 50% null values\n",
    "null_columns = list(null_percentages[null_percentages > 50].index)\n",
    "\n",
    "# Drop columns with more than 50% null values in X_train and X_test\n",
    "X_train = X_train.drop(null_columns, axis=1)\n",
    "X_test = X_test.drop(null_columns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENTO    0.809444\n",
      "CANAL       0.000000\n",
      "COD_PAIS    0.000000\n",
      "SEXO        1.854975\n",
      "Canal1      0.000000\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "SEGMENTO     6\n",
      "CANAL        3\n",
      "COD_PAIS    29\n",
      "SEXO         2\n",
      "Canal1       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of null values in each categorical column\n",
    "print(X_train.select_dtypes(include=['object']).isnull().sum() * 100. / X_train.shape[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check cardinality of categorical columns\n",
    "print(X_train.select_dtypes(include=['object']).nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENTO    0.0\n",
      "CANAL       0.0\n",
      "COD_PAIS    0.0\n",
      "SEXO        0.0\n",
      "Canal1      0.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "SEGMENTO    5\n",
      "CANAL       3\n",
      "COD_PAIS    8\n",
      "SEXO        2\n",
      "Canal1      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of null values in each categorical column\n",
    "print(X_test.select_dtypes(include=['object']).isnull().sum() * 100. / X_test.shape[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check cardinality of categorical columns\n",
    "print(X_test.select_dtypes(include=['object']).nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_binary_features = [\"SEXO\", \"Canal1\"]\n",
    "categorical_to_drop = \"COD_PAIS\"\n",
    "categorical_non_binary_features = [\"CANAL\", \"SEGMENTO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATM_INT' 'POS' 'MCI']\n",
      "['POS' 'MCI' 'ATM_INT']\n",
      "\n",
      "\n",
      "['Personal Plus' 'Personal' 'Emprendedor' nan 'PYME' 'Preferencial'\n",
      " 'Empresarial']\n",
      "['Personal Plus' 'Preferencial' 'Personal' 'Emprendedor' 'PYME']\n"
     ]
    }
   ],
   "source": [
    "# Print unique values of \"CANAL\" and \"SEGMENTO\" in X_train and X_test\n",
    "print(X_train[\"CANAL\"].unique())\n",
    "print(X_test[\"CANAL\"].unique())\n",
    "print(\"\\n\")\n",
    "print(X_train[\"SEGMENTO\"].unique())\n",
    "print(X_test[\"SEGMENTO\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute column \"SEGMENTO\" in X_train with the most frequent value\n",
    "X_train[\"SEGMENTO\"] = X_train[\"SEGMENTO\"].fillna(X_train[\"SEGMENTO\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index for X_test\n",
    "X_test_index = X_test.index\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column \"COD_PAIS\" from X_train and X_test\n",
    "X_train = X_train.drop(\"COD_PAIS\", axis=1)\n",
    "X_test = X_test.drop(\"COD_PAIS\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\Documents\\Work Search\\Cantilever application\\Data Science Test Solution\\datascience_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\cesar\\Documents\\Work Search\\Cantilever application\\Data Science Test Solution\\datascience_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply LabelEncoder to categorical features in categorical_binary_features\n",
    "le = LabelEncoder()\n",
    "\n",
    "for feature in categorical_binary_features:\n",
    "    X_train[feature] = le.fit_transform(X_train[feature])\n",
    "    X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "# Apply OneHotEncoder to categorical features in categorical_non_binary_features\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "for feature in categorical_non_binary_features:\n",
    "    X_train_ohe = ohe.fit_transform(X_train[[feature]])\n",
    "    X_test_ohe = ohe.transform(X_test[[feature]])\n",
    "\n",
    "    # Create column names for each OHE column\n",
    "    ohe_categories = [f'{feature}_{category}' for category in ohe.categories_[0]]\n",
    "\n",
    "    # Create a DataFrame with OHE columns\n",
    "    X_train_ohe = pd.DataFrame(X_train_ohe, columns=ohe_categories)\n",
    "    X_test_ohe = pd.DataFrame(X_test_ohe, columns=ohe_categories)\n",
    "\n",
    "    # Concatenate X_train_ohe to X_train and X_test_ohe to X_test\n",
    "    X_train = pd.concat([X_train, X_train_ohe], axis=1)\n",
    "    X_test = pd.concat([X_test, X_test_ohe], axis=1)\n",
    "\n",
    "    # Drop feature from X_train and X_test\n",
    "    X_train = X_train.drop(feature, axis=1)\n",
    "    X_test = X_test.drop(feature, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dist_Mean_NAL', 'EDAD', 'INGRESOS', 'EGRESOS', 'FECHA', 'VALOR',\n",
      "       'Dist_sum_NAL', 'Dist_HOY', 'OFICINA_VIN', 'HORA_AUX', 'DIAMES', 'SEXO',\n",
      "       'NROCIUDADES', 'NROPAISES', 'DIASEM', 'Canal1', 'FECHA_VIN',\n",
      "       'CANAL_ATM_INT', 'CANAL_MCI', 'CANAL_POS', 'SEGMENTO_Emprendedor',\n",
      "       'SEGMENTO_Empresarial', 'SEGMENTO_PYME', 'SEGMENTO_Personal',\n",
      "       'SEGMENTO_Personal Plus', 'SEGMENTO_Preferencial'],\n",
      "      dtype='object')\n",
      "Index(['Dist_Mean_NAL', 'EDAD', 'INGRESOS', 'EGRESOS', 'FECHA', 'VALOR',\n",
      "       'Dist_sum_NAL', 'Dist_HOY', 'OFICINA_VIN', 'HORA_AUX', 'DIAMES', 'SEXO',\n",
      "       'NROCIUDADES', 'NROPAISES', 'DIASEM', 'Canal1', 'FECHA_VIN',\n",
      "       'CANAL_ATM_INT', 'CANAL_MCI', 'CANAL_POS', 'SEGMENTO_Emprendedor',\n",
      "       'SEGMENTO_Empresarial', 'SEGMENTO_PYME', 'SEGMENTO_Personal',\n",
      "       'SEGMENTO_Personal Plus', 'SEGMENTO_Preferencial'],\n",
      "      dtype='object')\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Print columns in X_train\n",
    "print(X_train.columns)\n",
    "\n",
    "# Print columns in X_test\n",
    "print(X_test.columns)\n",
    "\n",
    "# Verify that the columns in X_train and X_test are the same and have the same order\n",
    "print(X_train.columns == X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming y_train are the labels for the training set\n",
    "model = XGBClassifier(objective='binary:logistic')  # for binary classification\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame y_test\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "# Add index to y_test using X_test_index\n",
    "y_test.index = X_test_index\n",
    "\n",
    "# Add column \"FRAUDE\" with the predicitions of the model using X_test\n",
    "y_test[\"FRAUDE\"] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import function1, function2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function is working\n"
     ]
    }
   ],
   "source": [
    "function1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
